\documentclass[fyp]{socreport}
\usepackage{fullpage}
\begin{document}
\pagenumbering{roman}
\title{Exploring Locality Sensitive Hashing}
\author{Govind Venugopalan}
\projyear{2019/2020}
\projnumber{H123456 (TO BE CHANGED)}
\advisor{Asst. Prof Harold Soh, Asst Prof Kuldeep S Meel, Asst Prof Jonathan Scarlett}
\deliverables{
	\item Report: 1 Volume
	\item Source Code: None}
\maketitle
\begin{abstract}
Data is in huge abundance today and has increasingly high dimension. Normal classical, exact and deterministic algorithms normally suffer from the curse of dimensionality which cause algorithms to scale in exponential time which is not beneficial

\begin{descriptors}
    \item C5 Computer System Implementation
	\item G2.2 Graph Algorithms
\end{descriptors}
\begin{keywords}
	Problem, algorithm, implementation
\end{keywords}
\begin{implement}
	Solaris 10, g++ 3.3, Tcl/Tk 8.4.7
\end{implement}
\end{abstract}

\begin{acknowledgement}
   I would like to thank my friends, families and advisors.
   Without them, I would not have be able to complete this project.
\end{acknowledgement}

\listoffigures 
\listoftables
\tableofcontents 

\chapter{Introduction}
\label{ch: intro}
Currently a lot of data processing involves crunching numbers with high dimensional data and ensuring that those 

\section{Background}
\label{sec : background}
In this section, we briefly discuss the history and background
of the problem.  A detail literature survey is presented in 
Chapter \ref{ch:related}.



\section{The Problem}
\label{sec: problem}
In this section, we formally defined the problem.  We adopt
the definition given by Kovsky \cite{kovsky92diff}.

\section{Our Solution}
\label{sec :solution}
\section{Report Organization}
\label{sec : org}
\chapter{Related Work}
\label{ch:related}

\chapter{Related Work}
\section{Classical Minwise Hashing}
\section{One Permutation Hashing}
\subsection{Introduction}
The first problem with regards to classical minwise hashing is that in the classical nearest-neighbours search algorithm, each query will take $O(dKL)$ time to calculate, where $d$ is the number of non-zero entries within the hash. This can be too expensive especially if $d$ is large since we already need $K$ and $L$ to be large in order for our accuracy to be good.\\

The main reason why the costs become so expensive is due to the fact that there many different permutations to consider and each permutation must be evaluated in order for min-wise hashing to work. Hence the proposal by \cite{ali94diff}
\subsection{Basis of Theoretical Analysis}

\section{Winner-Takes All Hashing}

\chapter{Improved Analysis}
\section{Gaps in previous work}
\section{Gaps to address in current work}
\section{Results}

\chapter{Conclusion}
\section{Contributions}
\section{Future Work}

\bibliographystyle{socreport}
\bibliography{socreport}

\appendix
\chapter{Code}
\chapter{Experimental Results}
\chapter{Proofs}
In this appendix we shall provide various extended proofs of lemmas found in the report. 
\end{document}
